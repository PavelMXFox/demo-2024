{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027341,
     "end_time": "2020-08-28T10:19:43.044804",
     "exception": false,
     "start_time": "2020-08-28T10:19:43.017463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Default Predict model Demo**  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ydata_profiling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from math import log as log\n",
    "import os\n",
    "import mlflow\n",
    "# Отключение ворнингов с самоподписными сертификатами для demo стенда\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# этот блок закомментирован так как используется только на kaggle\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "#PATH_to_file = '/kaggle/input/sf-dst-scoring/'\n",
    "\n",
    "# # # этот блок закомментирован так как используется только локальной машине\n",
    "from importlib import reload\n",
    "print(os.listdir('./data'))\n",
    "PATH_to_file = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.107129,
     "end_time": "2020-08-28T10:19:46.240948",
     "exception": false,
     "start_time": "2020-08-28T10:19:46.133819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils_module28072020 as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.754946,
     "end_time": "2020-08-28T10:19:49.014718",
     "exception": false,
     "start_time": "2020-08-28T10:19:46.259772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "!pip freeze > requirements.txt\n",
    "CURRENT_DATE = pd.to_datetime('11/08/2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017939,
     "end_time": "2020-08-28T10:19:49.051007",
     "exception": false,
     "start_time": "2020-08-28T10:19:49.033068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.27552,
     "end_time": "2020-08-28T10:19:49.344809",
     "exception": false,
     "start_time": "2020-08-28T10:19:49.069289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(PATH_to_file+'train.csv')\n",
    "df_test = pd.read_csv(PATH_to_file+'test.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print('Размерность тренировочного датасета: ', df_train.shape)\n",
    "display(df_train.head(2))\n",
    "print('Размерность тестового датасета: ', df_test.shape)\n",
    "display(df_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.129485,
     "end_time": "2020-08-28T10:19:49.492710",
     "exception": false,
     "start_time": "2020-08-28T10:19:49.363225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ВАЖНО! для корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['Train'] = 1 # помечаем где у нас трейн\n",
    "df_test['Train'] = 0 # помечаем где у нас тест\n",
    "\n",
    "df = df_train.append(df_test, sort=False).reset_index(drop=True) # объединяем\n",
    "#!Обратите внимание объединение датасетов является потенциальной опасностью для даталиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027649,
     "end_time": "2020-08-28T10:19:50.606276",
     "exception": false,
     "start_time": "2020-08-28T10:19:50.578627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # временной ряд (1)\n",
    "time_cols = ['app_date']\n",
    "# бинарные переменные (default не включаем в список) (5+1 = 6)\n",
    "bin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\n",
    "# категориальные переменные (Train не включаем в список, так как мы сами его добавили) (3+1=4)\n",
    "cat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\n",
    "# числовые переменные, client_id исключили из списка (8)\n",
    "num_cols = ['age','decline_app_cnt','score_bki','bki_request_cnt','income','days']\n",
    "# client_id не включаем в списки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приведение признаков к целевому виду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = np.log(df['age'] + 1)\n",
    "df['decline_app_cnt'] = np.log(df['decline_app_cnt'] + 1)\n",
    "df['bki_request_cnt'] = np.log(df['bki_request_cnt'] + 1)\n",
    "df['income'] = np.log(df['income'] + 1)\n",
    "df['education'] = df['education'].fillna('SCH')\n",
    "df.app_date = pd.to_datetime(df.app_date, format='%d%b%Y')\n",
    "\n",
    "start = df.app_date.min()\n",
    "end = df.app_date.max()\n",
    "df['days'] = (df.app_date - start).dt.days.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024728,
     "end_time": "2020-08-28T10:20:00.836879",
     "exception": false,
     "start_time": "2020-08-28T10:20:00.812151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "### Оценка корреляций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.475338,
     "end_time": "2020-08-28T10:20:01.337232",
     "exception": false,
     "start_time": "2020-08-28T10:20:00.861894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.simple_heatmap('Матрица корреляции тренировочного датасета на числовых переменных',df[df['Train']==1], num_cols+['default'], 1.1, 1, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026161,
     "end_time": "2020-08-28T10:20:01.389382",
     "exception": false,
     "start_time": "2020-08-28T10:20:01.363221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - сильно скорелированных между собой признаков нет, все берем в работу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025579,
     "end_time": "2020-08-28T10:20:01.440860",
     "exception": false,
     "start_time": "2020-08-28T10:20:01.415281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Значимость непрерывных переменных по ANOVA F test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.28713,
     "end_time": "2020-08-28T10:20:01.753661",
     "exception": false,
     "start_time": "2020-08-28T10:20:01.466531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_df = df[df['Train']==1]\n",
    "imp_num = pd.Series(f_classif(temp_df[num_cols], temp_df['default'])[0], index = num_cols)\n",
    "imp_num.sort_values(inplace = True)\n",
    "imp_num.plot(kind = 'barh', title='Значимость непрерывных переменных по ANOVA F test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025827,
     "end_time": "2020-08-28T10:20:01.806238",
     "exception": false,
     "start_time": "2020-08-28T10:20:01.780411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - оценка плательщика БКИ (score_bki) самый значимый показатель по ANOVA F test, потом кол-во отказанных заявок (declain_app_cnt) и в конце возраст (age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026235,
     "end_time": "2020-08-28T10:20:01.858837",
     "exception": false,
     "start_time": "2020-08-28T10:20:01.832602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Посмотрим на значимость категориальных и бинарных переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.593323,
     "end_time": "2020-08-28T10:20:02.478762",
     "exception": false,
     "start_time": "2020-08-28T10:20:01.885439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['education_l'] = label_encoder.fit_transform(df['education'])\n",
    "\n",
    "# паралельно подготовим бинарные переменные и переведем их в числовой формат\n",
    "# для бинарных признаков мы будем использовать LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for column in bin_cols:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "    \n",
    "# тут могут быть потенциальные даталики, но мы пока не придумали как это обработать,\n",
    "# потому что далее по этим меткам формируются новые фичи по get_dummies\n",
    "\n",
    "all_cat_and_bin_cols = cat_cols+bin_cols\n",
    "all_cat_and_bin_cols.remove('education')\n",
    "all_cat_and_bin_cols.append('education_l')\n",
    "print(all_cat_and_bin_cols)\n",
    "\n",
    "temp_df = df[df['Train']==1]\n",
    "imp_cat = pd.Series(mutual_info_classif(temp_df[all_cat_and_bin_cols], temp_df['default'], discrete_features =True), index = all_cat_and_bin_cols)\n",
    "imp_cat.sort_values(inplace = True)\n",
    "imp_cat.plot(kind = 'barh', title = 'Значимость бин. и категор. переменных по Mutual information test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026589,
     "end_time": "2020-08-28T10:20:02.532788",
     "exception": false,
     "start_time": "2020-08-28T10:20:02.506199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - Самым значимым признаком по Mutual information тесту является связь заемщика с клиентами банка (sna) и давность наличия информации о заемщике (first_time), потом идет рейтинг региона (region_rating) и в конце пол (sex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026612,
     "end_time": "2020-08-28T10:20:02.586455",
     "exception": false,
     "start_time": "2020-08-28T10:20:02.559843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Подготовка данных к машинному обучению\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.078306,
     "end_time": "2020-08-28T10:20:02.691527",
     "exception": false,
     "start_time": "2020-08-28T10:20:02.613221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# реализуем метод OneHotLabels через get_dummies\n",
    "df=pd.get_dummies(df, prefix=cat_cols, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02654,
     "end_time": "2020-08-28T10:20:02.745037",
     "exception": false,
     "start_time": "2020-08-28T10:20:02.718497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Стандартизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.155038,
     "end_time": "2020-08-28T10:20:02.926971",
     "exception": false,
     "start_time": "2020-08-28T10:20:02.771933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# стандартизацию проводим отдельно для трейна и теста, чтобы не допустить даталиков\n",
    "utils.StandardScaler_df_and_filna_0(df[df['Train']==1], num_cols)\n",
    "\n",
    "utils.StandardScaler_df_and_filna_0(df[df['Train']==0], num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02722,
     "end_time": "2020-08-28T10:20:03.119841",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.092621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Удаление нечисловых критериев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041296,
     "end_time": "2020-08-28T10:20:03.188533",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.147237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['app_date', 'education_l'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02661,
     "end_time": "2020-08-28T10:20:03.243125",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.216515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Построение модели\n",
    "---\n",
    "### Разбиваем датасет на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.074505,
     "end_time": "2020-08-28T10:20:03.344829",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.270324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = df.query('Train == 1').drop(['Train', 'client_id'], axis=1)\n",
    "test_data = df.query('Train == 0').drop(['Train', 'client_id'], axis=1)\n",
    "\n",
    "y = train_data.default.values            # наш таргет\n",
    "X = train_data.drop(['default'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.05414,
     "end_time": "2020-08-28T10:20:03.427129",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.372989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.036885,
     "end_time": "2020-08-28T10:20:03.491909",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.455024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём эксперимент в MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('default-predict') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027547,
     "end_time": "2020-08-28T10:20:03.547651",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.520104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Обучаем модель, генерируем результат и сравниваем с тестом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.843937,
     "end_time": "2020-08-28T10:20:04.419321",
     "exception": false,
     "start_time": "2020-08-28T10:20:03.575384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# первая модель на автомате\n",
    "model = LogisticRegression(random_state=RANDOM_SEED)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038658,
     "end_time": "2020-08-28T10:20:04.486104",
     "exception": false,
     "start_time": "2020-08-28T10:20:04.447446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Оценка качества модели\n",
    "---\n",
    "### Качественные метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в первый раз инициируем глобальную переменную с предыдущим скором\n",
    "utils.last_pred = np.zeros((3,len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.117417,
     "end_time": "2020-08-28T10:20:04.695723",
     "exception": false,
     "start_time": "2020-08-28T10:20:04.578306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.test_last_pred(y_test, y_pred, y_pred_prob) if (utils.last_pred[0].max() == 0) else 0\n",
    "utils.all_metrics(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027343,
     "end_time": "2020-08-28T10:20:04.750812",
     "exception": false,
     "start_time": "2020-08-28T10:20:04.723469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Матрица ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.242475,
     "end_time": "2020-08-28T10:20:05.021030",
     "exception": false,
     "start_time": "2020-08-28T10:20:04.778555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.confusion_matrix_f(['Дефолтный','Не дефолтный'], y_test, y_pred, 1.2, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028313,
     "end_time": "2020-08-28T10:20:05.077969",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.049656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ROC кривая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.30702,
     "end_time": "2020-08-28T10:20:05.413279",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.106259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.ROC_curve_with_area(y_test, y_pred_prob, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029481,
     "end_time": "2020-08-28T10:20:05.471805",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.442324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - прекрасный пример несостоятельности метрики ROC-AUC на не сбалансированных данных. Мы абсолютно не угадали дефолтных клиентов, тем самым обеспечили себе огромную ошибку второго рода и как следствие колосальный убыток, но ROC-AUC у нас высокий. Благо f1 как-то сигнализирует о том что что-то не впорядке. Надо посмотреть на метрику которая может оценивать эффективность алгоритма на несбалансированных данных - PRC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02883,
     "end_time": "2020-08-28T10:20:05.529690",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.500860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Precision-Recall кривая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.298146,
     "end_time": "2020-08-28T10:20:05.857158",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.559012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.PR_curve_with_area(y_test, y_pred, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение метрик, артефактов и графиков в MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version_info\n",
    "#import feast\n",
    "import sklearn\n",
    "import numpy\n",
    "import dill\n",
    "import joblib\n",
    "\n",
    "conda_env={\n",
    "    'channels': ['defaults'],\n",
    "    'dependencies': [\n",
    "      'python=3.8.10',\n",
    "      'pip>=22.0, <24.0',\n",
    "      'setuptools>=58.0, <72.0',\n",
    "      {\n",
    "        'pip': [\n",
    "          'mlflow=={}'.format(mlflow.__version__),\n",
    "          'numpy=={}'.format(numpy.__version__),\n",
    "          'scikit-learn=={}'.format(sklearn.__version__),\n",
    "          'joblib=={}'.format(joblib.__version__),\n",
    "          'dill=={}'.format(dill.__version__),\n",
    "        ],\n",
    "      },\n",
    "    ],\n",
    "    'name': 'demo_env'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = X_test.copy()\n",
    "eval_data[\"target\"] = y_test\n",
    "git_commit_id = os.popen('git rev-parse HEAD').read().strip('\\n')\n",
    "git_branch = os.popen('git rev-parse --abbrev-ref HEAD').read().strip('\\n')\n",
    "git_remote_url = os.popen('git remote get-url origin').read().strip('\\n')\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "   model_info = mlflow.sklearn.log_model(model, \"model\", conda_env=conda_env)\n",
    "   mlflow.set_tag(\"mlflow.source.type\", \"JOB\")\n",
    "   mlflow.set_tag(\"mlflow.source.name\", git_remote_url)\n",
    "   mlflow.set_tag(\"mlflow.source.git.commit\", git_commit_id)\n",
    "   mlflow.set_tag(\"mlflow.source.git.branch\", git_branch)\n",
    "   mlflow.set_tag(\"mlflow.source.git.repoURL\", git_remote_url)\n",
    "   result = mlflow.evaluate(\n",
    "       model_info.model_uri,\n",
    "       eval_data,\n",
    "       targets=\"target\",\n",
    "       model_type=\"classifier\",\n",
    "       dataset_name=\"default-predict\",\n",
    "       evaluators=\"default\",\n",
    "       evaluator_config={\"explainability_nsamples\": 1000},\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029095,
     "end_time": "2020-08-28T10:20:05.916542",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.887447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Кросс-валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.367947,
     "end_time": "2020-08-28T10:20:08.313993",
     "exception": false,
     "start_time": "2020-08-28T10:20:05.946046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_vec = cross_validate(model, X_test, y_test, cv=10, scoring='roc_auc', return_train_score=True)\n",
    "utils.vis_cross_val_score('ROC-AUC', temp_vec, 0.744846, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03096,
     "end_time": "2020-08-28T10:20:08.375462",
     "exception": false,
     "start_time": "2020-08-28T10:20:08.344502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - модель, которую мы получили, очень плохая, несмотря на то, что целевая метрика ROC-AUC достаточно высокая (=0.745).  \n",
    "Потому что:\n",
    "- Из матрицы ошибок видно, что мы почти не угадываем дефолтных клиентов (38 из 1789). Об этом также свидетествует метрика полнота recall = 0.020799, которая собственно описывает этот момент. Таким образом все клиенты попали в ошибку второго рода, а это очень плохо для банка, так как мы будем выдавать кредиты людям которые их не смогут вернуть - риск потерять все деньги. \n",
    "- Но целевая переменная площадь под ROC кривой оказалась не такой уж плохой. Дело в том, что эта кривая плохо оценивает эффективность алгоритма на несбалансированных данных, поэтому мы добавили Precision-Recall кривую и увидели что эффективность нашего алгоритма оставляет желать лучшего.  \n",
    "\n",
    "Кросс валидация показала дисперсию ошибки на тесте 0.014 и так как мы не использовали перемешивания, то можно сказать что модель лучше работает на крайних фолдах, чем на средних. В анализе признака data приводилась аналитика, которая говорила о том что вероятно в середине периода в банке проводилась какая-то акция и порог выдачи кредита снижался. Но мы в своей работе стремились создать надежный алгоритм который снижает ошибку второго рода (снижает потери банка) и достаточно стабильно работает (f1, PRC_AUC) при оптимальных значениях точности, а не гнались за просто высоким результатом на лидерборде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0297,
     "end_time": "2020-08-28T10:20:08.435268",
     "exception": false,
     "start_time": "2020-08-28T10:20:08.405568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Поиск оптимальных параметров модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 114.16859,
     "end_time": "2020-08-28T10:22:02.633779",
     "exception": false,
     "start_time": "2020-08-28T10:20:08.465189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# запускаем GridSearch на небольшом кол-ве итераций max_iter=50 и с достаточно большой дельтой останова tol1e-3\n",
    "# чтобы получить оптимальные параметры модели в первом приближении\n",
    "model = LogisticRegression(random_state=RANDOM_SEED)\n",
    "\n",
    "iter_ = 50\n",
    "epsilon_stop = 1e-3\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty': ['l1'], \n",
    "     'solver': ['liblinear', 'lbfgs'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "    {'penalty': ['l2'], \n",
    "     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "    {'penalty': ['none'], \n",
    "     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "]\n",
    "gridsearch = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "model = gridsearch.best_estimator_\n",
    "##печатаем параметры\n",
    "best_parameters = model.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    ##печатаем метрики\n",
    "preds = model.predict(X_test)\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, preds))\n",
    "print('Precision: %.4f' % precision_score(y_test, preds))\n",
    "print('Recall: %.4f' % recall_score(y_test, preds))\n",
    "print('F1: %.4f' % f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029554,
     "end_time": "2020-08-28T10:22:02.694505",
     "exception": false,
     "start_time": "2020-08-28T10:22:02.664951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - метрика f1 подросла почти в 10 раз, что вселяет надежду на то, что модель теперь будет работать эффективнее. Построим модель на этих параметрах и посмотрим, но сначала заново пересоберем трейн и тест перемешав выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.204831,
     "end_time": "2020-08-28T10:22:06.928987",
     "exception": false,
     "start_time": "2020-08-28T10:22:02.724156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вторая модель на параметрах после первой оптимизации\n",
    "model = LogisticRegression(random_state=RANDOM_SEED, \n",
    "                           C=1, \n",
    "                           class_weight= 'balanced', \n",
    "                           dual= False, \n",
    "                           fit_intercept= True, \n",
    "                           intercept_scaling= 1, \n",
    "                           l1_ratio= None, \n",
    "                           multi_class= 'auto', \n",
    "                           n_jobs= None, \n",
    "                           penalty= 'l2', \n",
    "                           solver = 'sag', \n",
    "                           verbose= 0, \n",
    "                           warm_start= False)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029931,
     "end_time": "2020-08-28T10:22:06.989513",
     "exception": false,
     "start_time": "2020-08-28T10:22:06.959582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Сохраняем данные о модели в MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "   model_info = mlflow.sklearn.log_model(model, \"model\", conda_env=conda_env)\n",
    "   result = mlflow.evaluate(\n",
    "       model_info.model_uri,\n",
    "       eval_data,\n",
    "       targets=\"target\",\n",
    "       model_type=\"classifier\",\n",
    "       dataset_name=\"default-predict\",\n",
    "       evaluators=\"default\",\n",
    "       evaluator_config={\"explainability_nsamples\": 1000},\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества модели на оптимальных параметрах "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.125768,
     "end_time": "2020-08-28T10:22:07.145669",
     "exception": false,
     "start_time": "2020-08-28T10:22:07.019901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.test_last_pred(y_test, y_pred, y_pred_prob) if (utils.last_pred[0].max() == 0) else 0\n",
    "utils.all_metrics(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030299,
     "end_time": "2020-08-28T10:22:07.206455",
     "exception": false,
     "start_time": "2020-08-28T10:22:07.176156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - все метрики подросли по сравнению с первой моделью, кроме accuracy и precision. Но зато сбалансированная accuracy выросла и мы можем ожидать не такой провал по ошибке второго рода как на первой модели. Ну и точность модели precision просела тоже в том числе потому что модель стала работать лучше и в FP полились клиенты из TN. Посмотрим матрицу ошибок и убедимся в этом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.265881,
     "end_time": "2020-08-28T10:22:07.502815",
     "exception": false,
     "start_time": "2020-08-28T10:22:07.236934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.confusion_matrix_f(['Дефолтный','Не дефолтный'], y_test, y_pred, 1.2, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.310047,
     "end_time": "2020-08-28T10:22:07.843846",
     "exception": false,
     "start_time": "2020-08-28T10:22:07.533799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.ROC_curve_with_area(y_test, y_pred_prob, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.298564,
     "end_time": "2020-08-28T10:22:08.175235",
     "exception": false,
     "start_time": "2020-08-28T10:22:07.876671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.PR_curve_with_area(y_test, y_pred, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031442,
     "end_time": "2020-08-28T10:22:08.238876",
     "exception": false,
     "start_time": "2020-08-28T10:22:08.207434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Резюме*** - эффективность алгоритма подросла и теперь более менее хорошо ведет себя на несбалансированных данных. Проверим это с помощью кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031523,
     "end_time": "2020-08-28T10:22:08.302473",
     "exception": false,
     "start_time": "2020-08-28T10:22:08.270950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Кросс-валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 8.724916,
     "end_time": "2020-08-28T10:22:17.059802",
     "exception": false,
     "start_time": "2020-08-28T10:22:08.334886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_vec = cross_validate(model, X_test, y_test, cv=10, scoring='roc_auc', return_train_score=True)\n",
    "utils.vis_cross_val_score('ROC-AUC', temp_vec, 0.744223, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032584,
     "end_time": "2020-08-28T10:22:17.126405",
     "exception": false,
     "start_time": "2020-08-28T10:22:17.093821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Резюме - Видно что при разбиении на 10 фолдов ROC-AUC меняется не сильно, есть один провал до 0.716. Среднее по фолдам примерно равно предсказанному значению до кроссвалидации 0.744, дисперсия 0.015, поэтому можно попробовать докрутить параметры и провести отбор признаков. Также можно попробовать прокрутить модель на регуляризации первого порядка l1 и попробовать избавиться от каких-нибудь признаков, которые не нравятся модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"data\": {\n",
    "    \"names\": [\"sex\", \"age\", \"car\", \"car_type\", \"decline_app_cnt\", \"good_work\",\n",
    "       \"score_bki\", \"bki_request_cnt\", \"income\", \"foreign_passport\", \"days\",\n",
    "       \"education_ACD\", \"education_GRD\", \"education_PGR\", \"education_SCH\",\n",
    "       \"education_UGR\", \"region_rating_20\", \"region_rating_30\",\n",
    "       \"region_rating_40\", \"region_rating_50\", \"region_rating_60\",\n",
    "       \"region_rating_70\", \"region_rating_80\", \"home_address_1\",\n",
    "       \"home_address_2\", \"home_address_3\", \"work_address_1\", \"work_address_2\",\n",
    "       \"work_address_3\", \"sna_1\", \"sna_2\", \"sna_3\", \"sna_4\", \"first_time_1\",\n",
    "       \"first_time_2\", \"first_time_3\", \"first_time_4\"],\n",
    "    \"tensor\": {\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        37\n",
    "      ],\n",
    "      \"values\": [\n",
    " 0,\n",
    " 3.4011973816621555,\n",
    " 0,\n",
    " 0,\n",
    " 0.6931471805599453,\n",
    " 0,\n",
    " 1.19598062,\n",
    " 1.3862943611198906,\n",
    " 10.043292972227004,\n",
    " 0,\n",
    " 73,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feast Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install feast==0.26.0 feast-cassandra psycopg2-binary dask[dataframe] boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "feature_store_folder_name = os.environ.get(\"PROJECT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from feast import (Entity, Field, FeatureView, ValueType, FeatureService)\n",
    "from feast.types import Int64, String, Float64\n",
    "from feast.infra.offline_stores.contrib.postgres_offline_store.postgres_source import (\n",
    "    PostgreSQLSource,\n",
    ")\n",
    "\n",
    "zipcode = Entity(name=\"zipcode\", value_type=ValueType.INT64)\n",
    "\n",
    "zipcode_source = PostgreSQLSource(\n",
    "    name=\"zipcode\",\n",
    "    query=\"SELECT * FROM zipcode\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "zipcode_features = FeatureView(\n",
    "    name=\"zipcode_features\",\n",
    "    entities=[zipcode],\n",
    "    ttl=timedelta(days=3650),\n",
    "    schema=[\n",
    "        Field(name=\"city\", dtype=String),\n",
    "        Field(name=\"state\", dtype=String),\n",
    "        Field(name=\"location_type\", dtype=String),\n",
    "        Field(name=\"tax_returns_filed\", dtype=Int64),\n",
    "        Field(name=\"population\", dtype=Int64),\n",
    "        Field(name=\"total_wages\", dtype=Int64),\n",
    "    ],\n",
    "    source=zipcode_source,\n",
    ")\n",
    "\n",
    "dob_ssn = Entity(\n",
    "    name=\"dob_ssn\",\n",
    "    value_type=ValueType.STRING,\n",
    "    description=\"Date of birth and last four digits of social security number\",\n",
    ")\n",
    "\n",
    "credit_history_source = PostgreSQLSource(\n",
    "    name=\"credit_history\",\n",
    "    query=\"SELECT * FROM credit_history\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "credit_history = FeatureView(\n",
    "    name=\"credit_history\",\n",
    "    entities=[dob_ssn],\n",
    "    ttl=timedelta(days=300),\n",
    "    schema=[\n",
    "        Field(name=\"credit_card_due\", dtype=Int64),\n",
    "        Field(name=\"mortgage_due\", dtype=Int64),\n",
    "        Field(name=\"student_loan_due\", dtype=Int64),\n",
    "        Field(name=\"vehicle_loan_due\", dtype=Int64),\n",
    "        Field(name=\"hard_pulls\", dtype=Int64),\n",
    "        Field(name=\"missed_payments_2y\", dtype=Int64),\n",
    "        Field(name=\"missed_payments_1y\", dtype=Int64),\n",
    "        Field(name=\"missed_payments_6m\", dtype=Int64),\n",
    "        Field(name=\"bankruptcies\", dtype=Int64),\n",
    "    ],\n",
    "    source=credit_history_source,\n",
    ")\n",
    "\n",
    "client_source = PostgreSQLSource(\n",
    "    name=\"client_stats\",\n",
    "    query=\"\"\"SELECT dob_ssn, CAST(date AS timestamp) AS event_timestamp, \n",
    "    SUM(income) OVER(\n",
    "        PARTITION BY dob_ssn\n",
    "        ORDER BY date ASC\n",
    "        RANGE BETWEEN INTERVAL '11' MONTH PRECEDING AND CURRENT ROW\n",
    "    ) AS incomeAmount12M,\n",
    "    MAX(date) OVER(\n",
    "        PARTITION BY dob_ssn\n",
    "        ORDER BY date ASC\n",
    "        RANGE BETWEEN INTERVAL '11' MONTH PRECEDING AND CURRENT ROW\n",
    "    ) AS created_timestamp\n",
    "    FROM income_history\"\"\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "id_entity = Entity(\n",
    "    name=\"dob_ssn\",\n",
    "    value_type=ValueType.STRING)\n",
    "\n",
    "scoring_features = FeatureView(\n",
    "    name=\"client_features\",\n",
    "    entities=[\n",
    "        id_entity,\n",
    "    ],\n",
    "    ttl=timedelta(days=3650),\n",
    "    schema=[\n",
    "        Field(name=\"incomeamount12m\", dtype=Float64),\n",
    "    ],\n",
    "    source=client_source,\n",
    ")\n",
    "\n",
    "feature_service = FeatureService(\n",
    "    name='loan_features',\n",
    "    features=[zipcode_features, credit_history, scoring_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with feast-hive\n",
    "features = f\"\"\"\n",
    "from datetime import timedelta\n",
    "\n",
    "from feast import (Entity, Field, FeatureView, ValueType, FeatureService)\n",
    "from feast.types import Int64, String, Float64\n",
    "from feast.infra.offline_stores.contrib.postgres_offline_store.postgres_source import (\n",
    "    PostgreSQLSource,\n",
    ")\n",
    "\n",
    "zipcode = Entity(name=\"zipcode\", value_type=ValueType.INT64)\n",
    "\n",
    "zipcode_source = PostgreSQLSource(\n",
    "    name=\"zipcode\",\n",
    "    query=\"SELECT * FROM zipcode\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "zipcode_features = FeatureView(\n",
    "    name=\"zipcode_features\",\n",
    "    entities=[zipcode],\n",
    "    ttl=timedelta(days=3650),\n",
    "    schema=[\n",
    "        Field(name=\"city\", dtype=String),\n",
    "        Field(name=\"state\", dtype=String),\n",
    "        Field(name=\"location_type\", dtype=String),\n",
    "        Field(name=\"tax_returns_filed\", dtype=Int64),\n",
    "        Field(name=\"population\", dtype=Int64),\n",
    "        Field(name=\"total_wages\", dtype=Int64),\n",
    "    ],\n",
    "    source=zipcode_source,\n",
    ")\n",
    "\n",
    "dob_ssn = Entity(\n",
    "    name=\"dob_ssn\",\n",
    "    value_type=ValueType.STRING,\n",
    "    description=\"Date of birth and last four digits of social security number\",\n",
    ")\n",
    "\n",
    "credit_history_source = PostgreSQLSource(\n",
    "    name=\"credit_history\",\n",
    "    query=\"SELECT * FROM credit_history\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "credit_history = FeatureView(\n",
    "    name=\"credit_history\",\n",
    "    entities=[dob_ssn],\n",
    "    ttl=timedelta(days=300),\n",
    "    schema=[\n",
    "        Field(name=\"credit_card_due\", dtype=Int64),\n",
    "        Field(name=\"mortgage_due\", dtype=Int64),\n",
    "        Field(name=\"student_loan_due\", dtype=Int64),\n",
    "        Field(name=\"vehicle_loan_due\", dtype=Int64),\n",
    "        Field(name=\"hard_pulls\", dtype=Int64),\n",
    "        Field(name=\"missed_payments_2y\", dtype=Int64),\n",
    "        Field(name=\"missed_payments_1y\", dtype=Int64),\n",
    "        Field(name=\"missed_payments_6m\", dtype=Int64),\n",
    "        Field(name=\"bankruptcies\", dtype=Int64),\n",
    "    ],\n",
    "    source=credit_history_source,\n",
    ")\n",
    "\n",
    "client_source = PostgreSQLSource(\n",
    "    name=\"client_stats\",\n",
    "    query=\\\"\\\"\\\"SELECT dob_ssn, CAST(date AS timestamp) AS event_timestamp, \n",
    "    SUM(income) OVER(\n",
    "        PARTITION BY dob_ssn\n",
    "        ORDER BY date ASC\n",
    "        RANGE BETWEEN INTERVAL '11' MONTH PRECEDING AND CURRENT ROW\n",
    "    ) AS incomeAmount12M,\n",
    "    MAX(date) OVER(\n",
    "        PARTITION BY dob_ssn\n",
    "        ORDER BY date ASC\n",
    "        RANGE BETWEEN INTERVAL '11' MONTH PRECEDING AND CURRENT ROW\n",
    "    ) AS created_timestamp\n",
    "    FROM income_history\\\"\\\"\\\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "id_entity = Entity(\n",
    "    name=\"dob_ssn\",\n",
    "    value_type=ValueType.STRING)\n",
    "\n",
    "scoring_features = FeatureView(\n",
    "    name=\"client_features\",\n",
    "    entities=[\n",
    "        id_entity,\n",
    "    ],\n",
    "    ttl=timedelta(days=3650),\n",
    "    schema=[\n",
    "        Field(name=\"incomeamount12m\", dtype=Float64),\n",
    "    ],\n",
    "    source=client_source,\n",
    ")\n",
    "\n",
    "feature_service = FeatureService(\n",
    "    name='loan_features',\n",
    "    features=[zipcode_features, credit_history, scoring_features]\n",
    ")\n",
    "\"\"\"\n",
    "with open(f'./{feature_store_folder_name}/features.py', \"w\") as features_file:\n",
    "    features_file.write(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "store = FeatureStore(repo_path=f\"./{feature_store_folder_name}\")\n",
    "store.apply([zipcode, zipcode_source, zipcode_features, dob_ssn, credit_history_source, credit_history,\n",
    "             client_source, id_entity, scoring_features, feature_service])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Get historic loan data\n",
    "loans = pd.read_parquet(\"loan_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_features = store.get_feature_service('loan_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = store.get_historical_features(\n",
    "    entity_df=loans, \n",
    "    features=loan_features\n",
    ")\n",
    "\n",
    "training_df = training_features.to_df()\n",
    "\n",
    "training_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "duration": 218.915575,
   "end_time": "2020-08-28T10:23:16.171872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-28T10:19:37.256297",
   "version": "2.1.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
